{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c59344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing as mp\n",
    "from IPython.core.debugger import set_trace\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import copy\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.datasets import make_swiss_roll,\\\n",
    "                             make_s_curve,\\\n",
    "                             make_moons\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from train_utils import *\n",
    "from metric_utils import calculate_Q_metrics, \\\n",
    "                         strain, \\\n",
    "                         l2_loss, \\\n",
    "                         to_numpy, \\\n",
    "                         numpy_metric, \\\n",
    "                         cosine_sim\n",
    "\n",
    "from input_utils import DataGenerator, make_random_affine\n",
    "from mlp_model import MLP_NonlinearEncoder\n",
    "from models_utils import init_weights, \\\n",
    "                         universal_approximator, \\\n",
    "                         dJ_criterion, \\\n",
    "                         gained_function, \\\n",
    "                         sigmoid, \\\n",
    "                         initialize_nonlinearities\n",
    "\n",
    "from embedding_utils import ConstructUMAPGraph, UMAPLoss, UMAPDataset, umap_criterion_compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "device = torch.device('cuda:0')\n",
    "N_CPU = mp.cpu_count()\n",
    "SEED = 42\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25495a50",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ed566",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = StandardScaler()\n",
    "\n",
    "input_parameters = {'generator': make_swiss_roll, #make_s_curve, \n",
    "                    'generator_kwargs': {'n_samples':10000, 'noise':1e-2}, # 1e-1\n",
    "                    'unsupervised':True,\n",
    "                    'whiten':True,\n",
    "                    'scaler':SCALER, #SCALER,\n",
    "                    'use_outpt_color':True} \n",
    "\n",
    "create_data = DataGenerator(**input_parameters)\n",
    "\n",
    "inpt, _, color = create_data()\n",
    "\n",
    "N_TEST = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt.mean(1), inpt.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7cb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt.max(1), inpt.min(1), inpt@inpt.T / inpt.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d81042",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_train, inpt_test, color_train, color_test = train_test_split(inpt.T, \n",
    "                                                                  color, \n",
    "                                                                  random_state=42,\n",
    "                                                                  test_size=N_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bf21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_train_torch = torch.tensor(inpt_train, dtype=torch.float32).to(device)\n",
    "inpt_test_torch = torch.tensor(inpt_test, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(inpt_train_torch.T@inpt_train_torch / inpt_train_torch.shape[0]).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5faa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_train_torch.mean(0), inpt_train_torch.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445616e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt_train_torch.shape, inpt_test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358c1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.ioff()\n",
    "# plt.figure()\n",
    "# df = pd.DataFrame(inpt.T, columns=['x','y', 'z'])\n",
    "# if color is not None:\n",
    "#     df['target'] = color\n",
    "# fig = px.scatter_3d(df, x='x', y='y', z='z', color='target' if 'target' in df else None)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861aa4fc",
   "metadata": {},
   "source": [
    "# Metalearning: gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d016a20",
   "metadata": {},
   "source": [
    "### Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_parameters = {\n",
    "                    'input_dim':inpt.shape[0],\n",
    "                    'hidden_dim':60,\n",
    "                    'embedding_dim':2,\n",
    "                    'add_readout':False,\n",
    "                    'add_recurrent_connections':False,\n",
    "                    'add_recurrent_nonlinearity':False,\n",
    "                    'hebbian_update':criterion_rule,\n",
    "                    'inplace_update':False,\n",
    "                    'normalize_hebbian_update':True,\n",
    "                    'lr_hebb':1e-5,\n",
    "                    'W_requires_grad':False,\n",
    "                    'W_r_requires_grad':False,\n",
    "                    'f_requires_grad':True,\n",
    "                    'final_nonlinearity':False,\n",
    "                    'parametrized_f':True,\n",
    "                    'nonlinearity': universal_approximator,\n",
    "#                     'nonlinearity': nn.Tanh(),\n",
    "                    'f_kwargs':{'hidden_dim':10, 'requires_grad':True},\n",
    "                    'layers_number':4,\n",
    "                    'add_bn':True,\n",
    "                    'seed':None,\n",
    "                    'set_seed':False,\n",
    "                     }\n",
    "\n",
    "network = MLP_NonlinearEncoder(**encoder_parameters).to(device)\n",
    "\n",
    "def weight_saver(network):\n",
    "    weights = {}\n",
    "    for k,v in network.named_parameters():\n",
    "        weights[k] = to_numpy(v.flatten())\n",
    "    return weights\n",
    "\n",
    "training_parameters = edict({'epochs':None,\n",
    "                           'enable_grad_train':True,\n",
    "                           'enable_grad_val':True,\n",
    "                           'backprop_learning':True,\n",
    "                           'hebbian_learning':False,\n",
    "                           'lr':1e-4,\n",
    "                           'wd':0, \n",
    "                           'maxiter':None, # maxiter\n",
    "                           'progress_bar':True,\n",
    "                           'weight_saver':None,\n",
    "                           'calculate_grad':True,\n",
    "                           'clip_grad_value': None,\n",
    "                           'val_metrics':None,\n",
    "                           'device':device\n",
    "                          })\n",
    "\n",
    "criterion_kwargs = defaultdict(dict)\n",
    "criterion_kwargs['skip_train'] = False\n",
    "criterion_kwargs['skip_val'] = False\n",
    "\n",
    "opt = None\n",
    "if training_parameters['backprop_learning']:\n",
    "    opt = optim.Adam(get_grad_params(network.parameters()), \n",
    "                     lr=training_parameters.lr,  \n",
    "                     weight_decay=training_parameters.wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc800e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Net capacity:', get_capacity(network))\n",
    "print('Parameters:')\n",
    "\n",
    "for name, param in network.named_parameters():\n",
    "    print(name, param.shape, 'requires_grad:', param.requires_grad, 'Device:', param.device)\n",
    "    W = to_numpy(param.data)\n",
    "    plt.hist(W.flatten(),bins=20, alpha=0.5, label=f'{name}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./results/mlp_nonlinear/swiss_roll/MLP_bp_hdim-60_lnum-4_Wgrad-1_fgrad-1_universal_approximator_bn-1')\n",
    "initialize_nonlinearities(network, state_dict)\n",
    "# network.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498a748",
   "metadata": {},
   "source": [
    "### Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54bee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_constructor = ConstructUMAPGraph(metric='euclidean', \n",
    "                                        n_neighbors=20, \n",
    "                                        random_state=SEED)\n",
    "\n",
    "# (epochs_per_sample, head, tail, weight) \n",
    "train_graph_data = graph_constructor(inpt_train)\n",
    "test_graph_data = graph_constructor(inpt_test)\n",
    "\n",
    "BATCH_SIZE_BP = 10000\n",
    "\n",
    "criterion_umap = UMAPLoss(device=device, \n",
    "                         min_dist=0.1,\n",
    "                         negative_sample_rate=5,\n",
    "                         edge_weight=None,\n",
    "                         repulsion_strength=1.0)\n",
    "\n",
    "criterion = umap_criterion_compatibility(criterion_umap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fb300f",
   "metadata": {},
   "source": [
    "### Create criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c19733",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = network.forward(inpt_test_torch.to(training_parameters['device']))\n",
    "outpt_val_pred = to_numpy(X_s[-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(outpt_val_pred[0],\n",
    "            outpt_val_pred[1], \n",
    "            c=color_test\n",
    "           )\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45339bd0",
   "metadata": {},
   "source": [
    "### Training Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64922771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network, _, metric_dict = train(network, \n",
    "#                               opt=None, \n",
    "#                               criterion=criterion,\n",
    "#                               criterion_kwargs=criterion_kwargs,\n",
    "#                               parameters=training_parameters,\n",
    "#                               train_dataloader=train_hebb_dataloader,\n",
    "#                               val_dataloader=dataset_test, \n",
    "#                               metric_dict=None,\n",
    "#                               val_metrics=None\n",
    "#                               )\n",
    "# plt.plot(metric_dict['criterion_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c001ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in network.state_dict().keys():\n",
    "#     if not 'f_s' in k:\n",
    "#         w_s = [w_dict[k] for w_dict in metric_dict['weights']]\n",
    "#         w_s = np.stack(w_s, 0)\n",
    "#         plt.figure()\n",
    "#         plt.plot(w_s)\n",
    "#         plt.title(k)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a901474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_s = network.forward(inpt_test_torch.to(training_parameters['device']))\n",
    "# outpt_val_pred = to_numpy(X_s[-1])\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(outpt_val_pred[0],\n",
    "#             outpt_val_pred[1], \n",
    "#             c=color_test\n",
    "#            )\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf395da",
   "metadata": {},
   "source": [
    "### Meta-iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(get_grad_params(network.parameters()),\n",
    "                 lr=training_parameters['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_state_dict = None\n",
    "best_metric = np.inf\n",
    "\n",
    "ITER_META = 50\n",
    "EPOCHS_META = 1 \n",
    "EPOCHS_HEBB = 1\n",
    "\n",
    "metric_dict = defaultdict(list)\n",
    "\n",
    "meta_switch_times = defaultdict(list)\n",
    "ordinary_switch_times = defaultdict(list)\n",
    "\n",
    "training_parameters['progress_bar'] = False\n",
    "training_parameters['weight_saver'] = None\n",
    "\n",
    "for meta_iter in tqdm(range(ITER_META)):\n",
    "    \n",
    "    print(f'Doing {meta_iter} META iteration')\n",
    "    \n",
    "    meta_switch_times['train'].append(len(metric_dict['criterion_train']))\n",
    "    meta_switch_times['val'].append(len(metric_dict['criterion_val']))\n",
    "    \n",
    "    # training meta-parameters usign BP\n",
    "    training_parameters['backprop_learning'] = True\n",
    "    training_parameters['hebbian_learning'] = False\n",
    "    \n",
    "    training_parameters['epochs'] = EPOCHS_META\n",
    "    criterion_kwargs['skip_train'] = False\n",
    "    criterion_kwargs['skip_val'] = False\n",
    "    \n",
    "    \n",
    "    if EPOCHS_META > 0:\n",
    "        \n",
    "        dataset_train = UMAPDataset(inpt_train, \n",
    "                                    *train_graph_data, \n",
    "                                    device=device, \n",
    "                                    batch_size=BATCH_SIZE_BP, \n",
    "                                    shuffle=True)\n",
    "        \n",
    "        dataset_test = UMAPDataset(inpt_test, \n",
    "                                   *test_graph_data, \n",
    "                                   device=device, \n",
    "                                   batch_size=BATCH_SIZE_BP, \n",
    "                                   shuffle=True)\n",
    "        \n",
    "        network, opt, metric_dict = train(network, \n",
    "                                      opt=opt, \n",
    "                                      criterion=criterion,\n",
    "                                      criterion_kwargs=criterion_kwargs,\n",
    "                                      parameters=training_parameters,\n",
    "                                      train_dataloader=dataset_train,\n",
    "                                      val_dataloader=dataset_test, \n",
    "                                      metric_dict=metric_dict,\n",
    "                                      val_metrics=None\n",
    "                                      )\n",
    "    \n",
    "                                  \n",
    "    ordinary_switch_times['train'].append(len(metric_dict['criterion_train']))\n",
    "    ordinary_switch_times['val'].append(len(metric_dict['criterion_val']))\n",
    "    \n",
    "    # save current model if there's improve\n",
    "    current_val_criterion = metric_dict['criterion_val'][-1]\n",
    "    if current_val_criterion < best_metric:\n",
    "        best_metric = current_val_criterion\n",
    "        best_state_dict = network.state_dict()\n",
    "    \n",
    "    \n",
    "    # training connectivity using local rule\n",
    "    training_parameters['backprop_learning'] = False\n",
    "    training_parameters['hebbian_learning'] = True\n",
    "    \n",
    "    training_parameters['batch_size'] = 1\n",
    "    training_parameters['epochs'] = EPOCHS_HEBB\n",
    "    criterion_kwargs['skip_train'] = True\n",
    "    criterion_kwargs['skip_val'] = False\n",
    "    \n",
    "    \n",
    "    # for hebbian update\n",
    "    train_hebb_dataloader = DataLoader(inpt_train_torch, batch_size=1, shuffle=True)\n",
    "\n",
    "    network, _, metric_dict = train(network, \n",
    "                                  opt=None, \n",
    "                                  criterion=criterion,\n",
    "                                  criterion_kwargs=criterion_kwargs,\n",
    "                                  parameters=training_parameters,\n",
    "                                  train_dataloader=train_hebb_dataloader,\n",
    "                                  val_dataloader=dataset_test, \n",
    "                                  metric_dict=metric_dict,\n",
    "                                  val_metrics=None\n",
    "                                  )\n",
    "    \n",
    "    # save current model if there's improve\n",
    "    current_val_criterion = metric_dict['criterion_val'][-1]\n",
    "    if current_val_criterion < best_metric:\n",
    "        best_metric = current_val_criterion\n",
    "        best_state_dict = network.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_state_dict(best_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83792b",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34dbd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = network.forward(inpt_test_torch.to(training_parameters['device']))\n",
    "outpt_val_pred = to_numpy(X_s[-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(outpt_val_pred[0],\n",
    "            outpt_val_pred[1], \n",
    "            c=color_test\n",
    "           )\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40870b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5), dpi=300)\n",
    "\n",
    "ax2 = plt.subplot(1,1,1)\n",
    "ax2.plot(metric_dict['criterion_val'])\n",
    "ax2.set_title('Criterion val')\n",
    "\n",
    "#####\n",
    "# cval_min = min(metric_dict['criterion_val'])\n",
    "# cval_max = max(metric_dict['criterion_val'])\n",
    "\n",
    "# ax2.vlines(meta_switch_times['val'], \\\n",
    "#            cval_min, \\\n",
    "#            cval_max, \\\n",
    "#            color='purple', alpha=0.5)\n",
    "\n",
    "# ax2.vlines(ordinary_switch_times['val'], \\\n",
    "#            cval_min, \\\n",
    "#            cval_max, \\\n",
    "#            color='green', alpha=0.5)\n",
    "####\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f61b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion_meta = defaultdict(list)\n",
    "# criterion_ordinary = defaultdict(list)\n",
    "\n",
    "# for phase in ['val']:\n",
    "#     for t in meta_switch_times[phase]:\n",
    "#         criterion_meta[phase] += metric_dict[f'criterion_{phase}'][t:t+EPOCHS_META] \n",
    "        \n",
    "#     for t in ordinary_switch_times[phase]:\n",
    "#         criterion_ordinary[phase] += metric_dict[f'criterion_{phase}'][t:t+EPOCHS_HEBB] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe74950",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.W_s[-1]@network.W_s[-1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64125bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fs = len(network.f_s)\n",
    "ξ = torch.linspace(-4,4,1000).to(device)\n",
    "\n",
    "for layer in range(n_fs):\n",
    "    f_theta = network.f_s[layer]\n",
    "    n_neurons = f_theta.input_dim\n",
    "    y = torch.stack([ξ for _ in range(n_neurons)],0) # [n_neurons, T]\n",
    "    \n",
    "    f = to_numpy(f_theta(y)) # [n_neurons, T]\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=n_neurons, nrows=1, figsize=(n_neurons*3,3))\n",
    "    \n",
    "    for j,ax in enumerate(axes):\n",
    "        ax.plot(to_numpy(ξ), f[j])\n",
    "        ax.set_title(f'Neuron: {j}')\n",
    "        \n",
    "    fig.suptitle(f'Layer: {layer}', y=1.1, color='blue')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539c057d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
